{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d67ea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-11-11T11:40:04.505Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 03:40:36.060458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trento\n",
      "True\n",
      "(819, 11, 11, 63)\n",
      "Test patch 1 shape  torch.Size([29395, 11, 11, 63])\n",
      "6\n",
      "WARNING:tensorflow:From /tmp/ipykernel_10929/2684546465.py:136: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/users/jgecvision/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 03:40:39.359299: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3783000000 Hz\n",
      "2021-11-11 03:40:39.366322: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x148881220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-11 03:40:39.366350: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-11-11 03:40:39.370349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-11 03:40:39.399778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-11 03:40:39.399798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1111]      \n",
      "2021-11-11 03:40:39.466478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1564] Found device 0 with properties: \n",
      "pciBusID: 0035:04:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.50GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-11-11 03:40:39.466520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-11 03:40:39.466581: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-11 03:40:39.466619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-11 03:40:39.466656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-11 03:40:39.469092: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-11 03:40:39.469141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-11 03:40:39.469179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-11 03:40:39.473393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1706] Adding visible gpu devices: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 11, 11, 63)]      0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 11, 11, 64)        36352     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "spectral_morph (SpectralMorp (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "spatial_morph (SpatialMorph) (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 11, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 11, 11, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 11, 11, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 11, 11, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 512)         590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 816,134\n",
      "Trainable params: 813,958\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "Train on 819 samples, validate on 29395 samples\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 03:40:42.212697: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14a497b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2021-11-11 03:40:42.212828: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2021-11-11 03:40:42.215144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1564] Found device 0 with properties: \n",
      "pciBusID: 0035:04:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.50GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-11-11 03:40:42.215177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-11 03:40:42.215193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-11 03:40:42.215206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-11 03:40:42.215218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-11 03:40:42.215267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-11 03:40:42.215283: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-11 03:40:42.215296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-11 03:40:42.219447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1706] Adding visible gpu devices: 0\n",
      "2021-11-11 03:40:44.122556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-11 03:40:44.122753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1111]      0 \n",
      "2021-11-11 03:40:44.122772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1124] 0:   N \n",
      "2021-11-11 03:40:44.127540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1250] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 29696 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0035:04:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "819/819 [==============================] - 11s 13ms/sample - loss: 1.0501 - accuracy: 0.6654 - val_loss: 1.6717 - val_accuracy: 0.4547\n",
      "Epoch 2/200\n",
      "819/819 [==============================] - 9s 11ms/sample - loss: 0.3356 - accuracy: 0.9170 - val_loss: 1.5281 - val_accuracy: 0.5744\n",
      "Epoch 3/200\n",
      "819/819 [==============================] - 9s 11ms/sample - loss: 0.2160 - accuracy: 0.9475 - val_loss: 1.2191 - val_accuracy: 0.4414\n",
      "Epoch 4/200\n",
      "819/819 [==============================] - 9s 11ms/sample - loss: 0.1403 - accuracy: 0.9634 - val_loss: 1.2573 - val_accuracy: 0.5101\n",
      "Epoch 5/200\n",
      "819/819 [==============================] - 9s 11ms/sample - loss: 0.0801 - accuracy: 0.9878 - val_loss: 1.7978 - val_accuracy: 0.2250\n",
      "Epoch 6/200\n",
      "819/819 [==============================] - 12s 14ms/sample - loss: 0.0538 - accuracy: 0.9939 - val_loss: 2.0616 - val_accuracy: 0.2258\n",
      "Epoch 7/200\n",
      "819/819 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9927"
     ]
    }
   ],
   "source": [
    "############ IMPORTS ####################\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0,'/home/users/jgecvision/.conda/envs/ankur01/lib/python3.8/site-packages')\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.utils.data as dataf\n",
    "from scipy import io\n",
    "import sys\n",
    "import spectral\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "sys.path.append(\"./../\")\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "import torch\n",
    "from operator import truediv\n",
    "import record\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.engine.topology import Layer\n",
    "from tensorflow.keras import initializers, constraints\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.utils import conv_utils\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import InputSpec, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Reshape, BatchNormalization, Layer, Dropout\n",
    "from tensorflow.keras.layers import Dropout, Input, LeakyReLU, Multiply\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Add, Activation, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical as keras_to_categorical\n",
    "\n",
    "\n",
    "############# CONFIGS ##########################\n",
    "\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "datasetNames = [\"Trento\"]\n",
    "testSizeNumber = 5000\n",
    "patchsize1 = 11\n",
    "patchsize2 = 11\n",
    "batchsize = 64\n",
    "EPOCH = 200\n",
    "LR = 0.001\n",
    "\n",
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc*100, average_acc*100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for datasetName in datasetNames:\n",
    "       \n",
    "        print(\"----------------------------------Training for \",datasetName,\"---------------------------------------------\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(datasetName)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        # Train data\n",
    "        HSI = io.loadmat('./../'+datasetName+'11x11/HSI_Tr.mat')\n",
    "        TrainPatch = HSI['Data']\n",
    "        TrainPatch = TrainPatch.astype(np.float32)\n",
    "        NC = TrainPatch.shape[3] # NC is number of bands\n",
    "\n",
    "        label = io.loadmat('./../'+datasetName+'11x11/TrLabel.mat')\n",
    "        TrLabel = label['Data']\n",
    "\n",
    "        # Test data\n",
    "        HSI = io.loadmat('./../'+datasetName+'11x11/HSI_Te.mat')\n",
    "        TestPatch = HSI['Data']\n",
    "        TestPatch = TestPatch.astype(np.float32)\n",
    "\n",
    "        label = io.loadmat('./../'+datasetName+'11x11/TeLabel.mat')\n",
    "        TsLabel = label['Data']\n",
    "\n",
    "\n",
    "        TrainPatch1 = torch.from_numpy(TrainPatch)\n",
    "    #         TrainPatch1 = TrainPatch1.permute(0,3,1,2)\n",
    "        TrainLabel1 = torch.from_numpy(TrLabel)-1\n",
    "        TrainLabel1 = TrainLabel1.long()\n",
    "\n",
    "\n",
    "        TestPatch1 = torch.from_numpy(TestPatch)\n",
    "    #         TestPatch1 = TestPatch1.permute(0,3,1,2)\n",
    "        TestLabel1 = torch.from_numpy(TsLabel)-1\n",
    "        TestLabel1 = TestLabel1.long()\n",
    "\n",
    "        Classes = len(np.unique(TrainLabel1))\n",
    "        TrainPatch1 = TrainPatch1.reshape(TrainPatch1.shape[0],TrainPatch1.shape[1]*TrainPatch1.shape[2],TrainPatch1.shape[3])\n",
    "        TestPatch1 = TestPatch1.reshape(TestPatch1.shape[0],TestPatch1.shape[1]*TestPatch1.shape[2],TestPatch1.shape[3])\n",
    "        print(\"Train data shape = \", TrainPatch1.shape)\n",
    "        print(\"Train label shape = \", TrainLabel1.shape)\n",
    "        print(\"Test data shape = \", TestPatch1.shape)\n",
    "        print(\"Test label shape = \", TestLabel1.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        KAPPA = []\n",
    "        OA = []\n",
    "        AA = []\n",
    "        ELEMENT_ACC = np.zeros((3, Classes))\n",
    "        tf.compat.v1.keras.backend.clear_session()\n",
    "        config = tf.compat.v1.ConfigProto( device_count = {'GPU': 0} ) \n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.compat.v1.Session(config=config) \n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            filters = 64\n",
    "            class Erosion2D(Layer):\n",
    "                '''\n",
    "                Erosion 2D Layer\n",
    "                for now assuming channel last\n",
    "                '''\n",
    "\n",
    "                def __init__(self, num_filters, kernel_size, strides=(1, 1),\n",
    "                             padding='same', kernel_initializer='glorot_uniform',\n",
    "                             kernel_constraint=None,\n",
    "                             **kwargs):\n",
    "                    super(Erosion2D, self).__init__(**kwargs)\n",
    "                    self.num_filters = num_filters\n",
    "                    self.kernel_size = kernel_size\n",
    "                    self.strides = strides\n",
    "                    self.padding = padding\n",
    "\n",
    "                    self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "                    self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "                    # for we are assuming channel last\n",
    "                    self.channel_axis = -1\n",
    "\n",
    "                    # self.output_dim = output_dim\n",
    "\n",
    "                def build(self, input_shape):\n",
    "                    if input_shape[self.channel_axis] is None:\n",
    "                        raise ValueError('The channel dimension of the inputs '\n",
    "                                         'should be defined. Found `None`.')\n",
    "\n",
    "                    input_dim = input_shape[self.channel_axis]\n",
    "                    kernel_shape = self.kernel_size + (input_dim, self.num_filters)\n",
    "\n",
    "                    self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                                  initializer=self.kernel_initializer,\n",
    "                                                  name='kernel',\n",
    "                                                  constraint=self.kernel_constraint)\n",
    "\n",
    "                    # Be sure to call this at the end\n",
    "                    super(Erosion2D, self).build(input_shape)\n",
    "                    self.input_spec = InputSpec(ndim=4,\n",
    "                                                axes={3: input_shape[-1]})\n",
    "                    self.built = True\n",
    "\n",
    "                def call(self, x):\n",
    "                    outputs = self.erosion2d(x, self.kernel[...,1],\n",
    "                                             self.strides, self.padding)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "                def erosion2d(self, x, st_element, strides, padding,\n",
    "                                rates=(1, 1, 1, 1)):\n",
    "                    # tf.nn.erosion2d(input, filter, strides, rates, padding, name=None)\n",
    "                    x = tf.compat.v1.nn.erosion2d(x, st_element, (1, ) + strides + (1, ),\n",
    "                                        rates, padding.upper())\n",
    "                    return x\n",
    "\n",
    "            class Dilation2D(Layer):\n",
    "                '''\n",
    "                Dilation 2D Layer\n",
    "                for now assuming channel last\n",
    "                '''\n",
    "\n",
    "                def __init__(self, num_filters, kernel_size, strides=(1, 1),\n",
    "                             padding='same', kernel_initializer='glorot_uniform',\n",
    "                             kernel_constraint=None,\n",
    "                             **kwargs):\n",
    "                    super(Dilation2D, self).__init__(**kwargs)\n",
    "                    self.num_filters = num_filters\n",
    "                    self.kernel_size = kernel_size\n",
    "                    self.strides = strides\n",
    "                    self.padding = padding\n",
    "\n",
    "                    self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "                    self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "                    # for we are assuming channel last\n",
    "                    self.channel_axis = -1\n",
    "\n",
    "                    # self.output_dim = output_dim\n",
    "\n",
    "                def build(self, input_shape):\n",
    "                    if input_shape[self.channel_axis] is None:\n",
    "                        raise ValueError('The channel dimension of the inputs '\n",
    "                                         'should be defined. Found `None`.')\n",
    "\n",
    "                    input_dim = input_shape[self.channel_axis]\n",
    "                    kernel_shape = self.kernel_size + (input_dim, self.num_filters)\n",
    "\n",
    "                    self.kernel = self.add_weight(shape=kernel_shape,\n",
    "                                                  initializer=self.kernel_initializer,\n",
    "                                                  name='kernel',\n",
    "                                                  constraint=self.kernel_constraint)\n",
    "\n",
    "                    # Be sure to call this at the end\n",
    "                    super(Dilation2D, self).build(input_shape)\n",
    "                    self.input_spec = InputSpec(ndim=4,\n",
    "                                                axes={3: input_shape[-1]})\n",
    "                    self.built = True\n",
    "\n",
    "                def call(self, x):\n",
    "                    outputs = self.dilation2d(x, self.kernel[...,1],\n",
    "                                             self.strides, self.padding)\n",
    "\n",
    "                    return outputs\n",
    "\n",
    "                def dilation2d(self, x, st_element, strides, padding,\n",
    "                                rates=(1, 1, 1, 1)):\n",
    "                    # tf.nn.erosion2d(input, filter, strides, rates, padding, name=None)\n",
    "                    x = tf.compat.v1.nn.dilation2d(x, st_element, (1, ) + strides + (1, ),\n",
    "                                        rates, padding.upper())\n",
    "                    return x\n",
    "\n",
    "\n",
    "            class SpectralMorph(Layer):\n",
    "                def __init__(self):\n",
    "\n",
    "                    super(SpectralMorph, self).__init__()\n",
    "                    self.filters = filters\n",
    "                    num_filters = filters\n",
    "                    #self.init = RandomNormal()\n",
    "                def call(self, x):\n",
    "                    z1 = Erosion2D(num_filters = self.filters, kernel_size = (3,3),padding=\"same\", strides=(1,1))(x)\n",
    "                    z1 = Conv2D(filters = self.filters, kernel_size = (1,1), padding='same')(z1)\n",
    "                    z2 = Dilation2D(num_filters = self.filters, kernel_size = (3,3), padding=\"same\", strides=(1,1))(x)\n",
    "                    z2 = Conv2D(filters = self.filters, kernel_size = (1,1), padding='same')(z2)\n",
    "                    x = Add()([z1, z2])\n",
    "                    return x\n",
    "\n",
    "\n",
    "            class SpatialMorph(Layer):\n",
    "                def __init__(self):\n",
    "\n",
    "                    super(SpatialMorph, self).__init__()\n",
    "                    self.filters = filters\n",
    "                    num_filters = filters\n",
    "                    #self.init = RandomNormal()\n",
    "                def call(self, x):\n",
    "                    z1 = Erosion2D(num_filters = self.filters, kernel_size = (3,3),padding=\"same\", strides=(1,1))(x)\n",
    "                    z1 = Conv2D(filters = self.filters, kernel_size = (3,3), padding='same')(z1)\n",
    "                    z2 = Dilation2D(num_filters = self.filters, kernel_size = (3,3), padding=\"same\", strides=(1,1))(x)\n",
    "                    z2 = Conv2D(filters = self.filters, kernel_size = (3,3), padding='same')(z2)\n",
    "                    x = Add()([z1, z2])\n",
    "                    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for iter in range(3):\n",
    "        #             tf.compat.v1.set_random_seed(43)\n",
    "        #             np.random.seed(43)\n",
    "\n",
    "                input_layer = Input((11, 11,NC))\n",
    "\n",
    "                z = Conv2D(filters, use_bias = True, kernel_size = 3, padding = 'same')(input_layer)\n",
    "                z = BatchNormalization()(z)\n",
    "                z = Activation('relu')(z)\n",
    "\n",
    "                z = SpectralMorph()(z)\n",
    "                z = BatchNormalization()(z)\n",
    "                z = Activation('relu')(z)\n",
    "\n",
    "                z = Conv2D(64, use_bias = True, kernel_size = 3, padding = 'same')(z)\n",
    "                z = BatchNormalization()(z)\n",
    "                z = Activation('relu')(z)\n",
    "\n",
    "\n",
    "                z = SpatialMorph()(z)\n",
    "                z = BatchNormalization()(z)\n",
    "                z = Activation('relu')(z)\n",
    "\n",
    "    #             z = MaxPooling2D(pool_size=(2,2), strides =1, padding = 'same')(z)\n",
    "\n",
    "                z = Conv2D(128, use_bias = True, kernel_size = 3, padding = 'same')(z)\n",
    "                z = BatchNormalization()(z)\n",
    "                z = Activation('relu')(z)\n",
    "    #             z = keras.layers.Concatenate(axis=3)([zspec,zspat])\n",
    "\n",
    "                z = MaxPooling2D(pool_size=(2,2), strides =2, padding = 'same')(z)\n",
    "\n",
    "                z = Conv2D(512, use_bias = True, kernel_size = 3, padding = 'same')(z)\n",
    "                z = BatchNormalization()(z)\n",
    "                z = Activation('relu')(z)\n",
    "                #z = Dropout(0.5)(z)\n",
    "\n",
    "                #z = Flatten()(z)\n",
    "                z = GlobalAveragePooling2D()(z)\n",
    "                #z = Dropout(0.3)(z)\n",
    "                z = Dense(128, activation=\"relu\")(z)\n",
    "                z = BatchNormalization()(z)\n",
    "                z = Dropout(0.5)(z)\n",
    "                z = Dense(64, activation=\"relu\")(z)\n",
    "                z = BatchNormalization()(z)\n",
    "                z = Dropout(0.5)(z)\n",
    "                output_layer = Dense(Classes, activation=\"softmax\")(z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                clf = Model(inputs=input_layer, outputs=output_layer)\n",
    "                clf.summary()\n",
    "                clf.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "                valdata = (TestPatch1.cpu().detach().numpy(), keras_to_categorical(TestLabel1.reshape(-1).cpu().detach().numpy(),Classes))\n",
    "                h = clf.fit(TrainPatch1.cpu().detach().numpy(), keras_to_categorical(TrainLabel1.reshape(-1).cpu().detach().numpy(),Classes),\n",
    "                                    batch_size=batchsize,\n",
    "                                    epochs=EPOCH,\n",
    "                                    verbose=True,\n",
    "                                    validation_data=valdata,\n",
    "                                    workers=4,\n",
    "                                    callbacks = [ModelCheckpoint(datasetName+\"/best_model_HSIOnly.h5\", monitor='val_accuracy', verbose=0, save_best_only=True,save_weights_only=True)])\n",
    "\n",
    "\n",
    "\n",
    "                clf.load_weights(datasetName+\"/best_model_HSIOnly.h5\")\n",
    "                pred_y = np.argmax(clf.predict(TestPatch1.cpu().detach().numpy()), axis=1)\n",
    "\n",
    "                y_test = TestLabel1.reshape(-1).cpu().detach().numpy()\n",
    "                oa = accuracy_score(y_test, pred_y)*100\n",
    "                confusion = confusion_matrix(y_test, pred_y)\n",
    "                each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "                kappa = cohen_kappa_score(y_test, pred_y)*100\n",
    "                KAPPA.append(kappa)\n",
    "                OA.append(oa)\n",
    "                AA.append(aa)\n",
    "                ELEMENT_ACC[iter, :] = each_acc\n",
    "\n",
    "        print(\"--------\" + datasetName + \" Training Finished-----------\")\n",
    "        record.record_output(OA, AA, KAPPA, ELEMENT_ACC,'./' + datasetName +'/MorphCNN_Report_' + datasetName +'.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa271c91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T14:12:00.148736Z",
     "start_time": "2021-11-11T14:11:58.592494Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-11 06:11:58.642248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1564] Found device 0 with properties: \n",
      "pciBusID: 0035:04:00.0 name: Tesla V100-SXM2-32GB computeCapability: 7.0\n",
      "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 31.50GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2021-11-11 06:11:58.661769: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-11-11 06:11:58.667346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-11 06:11:58.667395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-11 06:11:58.667419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-11 06:11:58.758397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-11 06:11:58.758568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-11 06:11:58.758607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-11 06:11:58.761774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1706] Adding visible gpu devices: 0\n",
      "2021-11-11 06:11:58.762037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-11 06:11:58.762055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1111]      0 \n",
      "2021-11-11 06:11:58.762068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1124] 0:   N \n",
      "2021-11-11 06:11:58.764657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1250] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 29696 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0035:04:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: conv2d_16/kernel:0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AssignVariableOp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         tld.op_callbacks, resource, value)\n\u001b[0m\u001b[1;32m    143\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: This function does not handle the case of the path where all inputs are not already EagerTensors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10929/1305973327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/best_model_HSIOnly_Deviation.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    231\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    232\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    249\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   def compile(self,\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1271\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    705\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[1;32m    706\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m   \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3382\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3383\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3384\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3385\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3386\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m--> 848\u001b[0;31m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[1;32m    849\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m    145\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         return assign_variable_op_eager_fallback(\n\u001b[0;32m--> 147\u001b[0;31m             resource, value, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    148\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op_eager_fallback\u001b[0;34m(resource, value, name, ctx)\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m   _result = _execute.execute(b\"AssignVariableOp\", 0, inputs=_inputs_flat,\n\u001b[0;32m--> 165\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    166\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     73\u001b[0m           \u001b[0;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: conv2d_16/kernel:0"
     ]
    }
   ],
   "source": [
    " clf.load_weights(datasetName+\"/best_model_HSIOnly_Deviation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0f65ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T14:25:53.038521Z",
     "start_time": "2021-11-11T14:25:52.954438Z"
    }
   },
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    clf.save_weights(datasetName+\"/best_model_HSIOnly_Deviation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69e2476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T14:13:07.671867Z",
     "start_time": "2021-11-11T14:13:06.636678Z"
    }
   },
   "outputs": [],
   "source": [
    "with g.as_default():\n",
    "    clf.save(datasetName+\"/best_model_HSIOnly_Deviation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ffc7be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T14:13:00.816471Z",
     "start_time": "2021-11-11T14:13:00.786897Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 0 layers into a model with 15 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10929/4100435584.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/best_model_HSIOnly_Deviation.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    231\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    232\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    248\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    249\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   def compile(self,\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   1271\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/purb37/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    683\u001b[0m                      \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                      \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m                      ' layers.')\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m   \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 0 layers into a model with 15 layers."
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    clf.load_weights(datasetName+\"/best_model_HSIOnly_Deviation.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
