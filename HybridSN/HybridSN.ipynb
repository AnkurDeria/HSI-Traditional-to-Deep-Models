{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7073a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Re-implementation for the paper \"HybridSN: Exploring 3-Dâ€“2-D CNN Feature Hierarchy for Hyperspectral Image Classification\"\n",
    "The official implementation is in https://github.com/gokriznastic/HybridSN\n",
    "'''\n",
    "\n",
    "############ IMPORTS ####################\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.utils.data as dataf\n",
    "from scipy import io\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "import torch\n",
    "from operator import truediv\n",
    "import record\n",
    "import tensorflow.keras as keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization\n",
    "from keras.layers import Dropout, Input\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical as keras_to_categorical\n",
    "\n",
    "\n",
    "############# CONFIGS ##########################\n",
    "\n",
    "# tf.config.experimental_run_functions_eagerly(True)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "datasetNames = [\"Trento\"]\n",
    "testSizeNumber = 5000\n",
    "patchsize1 = 11\n",
    "patchsize2 = 11\n",
    "batchsize = 64\n",
    "EPOCH = 200\n",
    "LR = 0.001\n",
    "\n",
    "def AA_andEachClassAccuracy(confusion_matrix):\n",
    "    counter = confusion_matrix.shape[0]\n",
    "    list_diag = np.diag(confusion_matrix)\n",
    "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
    "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
    "    average_acc = np.mean(each_acc)\n",
    "    return each_acc*100, average_acc*100\n",
    "\n",
    "def get_model_compiled(NC,Classes):\n",
    "    ## input layer\n",
    "    input_layer = Input((patchsize1, patchsize2, NC, 1))\n",
    "\n",
    "    ## convolutional layers\n",
    "    conv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 7), activation='relu')(input_layer)\n",
    "    conv_layer2 = Conv3D(filters=16, kernel_size=(3, 3, 5), activation='relu')(conv_layer1)\n",
    "    conv_layer3 = Conv3D(filters=32, kernel_size=(3, 3, 3), activation='relu')(conv_layer2)\n",
    "\n",
    "    conv3d_shape = conv_layer3.shape\n",
    "    conv_layer3 = Reshape((conv3d_shape[1], conv3d_shape[2], conv3d_shape[3]*conv3d_shape[4]))(conv_layer3)\n",
    "    conv_layer4 = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(conv_layer3)\n",
    "\n",
    "    flatten_layer = Flatten()(conv_layer4)\n",
    "\n",
    "    ## fully connected layers\n",
    "    dense_layer1 = Dense(units=256, activation='relu')(flatten_layer)\n",
    "    dense_layer1 = Dropout(0.4)(dense_layer1)\n",
    "    dense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\n",
    "    dense_layer2 = Dropout(0.4)(dense_layer2)\n",
    "    output_layer = Dense(units=Classes, activation='softmax')(dense_layer2)\n",
    "\n",
    "    clf = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    adam = Adam(lr=LR, decay=1e-06)\n",
    "    clf.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return clf\n",
    "\n",
    "\n",
    "for datasetName in datasetNames:\n",
    "        try:\n",
    "            os.makedirs(datasetName)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        print(\"----------------------------------Training for \",datasetName,\"---------------------------------------------\")\n",
    "\n",
    "        try:\n",
    "            os.makedirs(datasetName)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        # Train data\n",
    "        HSI = io.loadmat('./../'+datasetName+'11x11/HSI_Tr.mat')\n",
    "        TrainPatch = HSI['Data']\n",
    "        TrainPatch = TrainPatch.astype(np.float32)\n",
    "        NC = TrainPatch.shape[3] # NC is number of bands\n",
    "\n",
    "        label = io.loadmat('./../'+datasetName+'11x11/TrLabel.mat')\n",
    "        TrLabel = label['Data']\n",
    "\n",
    "        # Test data\n",
    "        HSI = io.loadmat('./../'+datasetName+'11x11/HSI_Te.mat')\n",
    "        TestPatch = HSI['Data']\n",
    "        TestPatch = TestPatch.astype(np.float32)\n",
    "\n",
    "        label = io.loadmat('./../'+datasetName+'11x11/TeLabel.mat')\n",
    "        TsLabel = label['Data']\n",
    "\n",
    "\n",
    "        TrainPatch1 = torch.from_numpy(TrainPatch)\n",
    "    #         TrainPatch1 = TrainPatch1.permute(0,3,1,2)\n",
    "        TrainLabel1 = torch.from_numpy(TrLabel)-1\n",
    "        TrainLabel1 = TrainLabel1.long()\n",
    "\n",
    "\n",
    "        TestPatch1 = torch.from_numpy(TestPatch)\n",
    "    #         TestPatch1 = TestPatch1.permute(0,3,1,2)\n",
    "        TestLabel1 = torch.from_numpy(TsLabel)-1\n",
    "        TestLabel1 = TestLabel1.long()\n",
    "\n",
    "        Classes = len(np.unique(TrainLabel1))\n",
    "        \n",
    "        print(\"Train data shape = \", TrainPatch1.shape)\n",
    "        print(\"Train label shape = \", TrainLabel1.shape)\n",
    "        print(\"Test data shape = \", TestPatch1.shape)\n",
    "        print(\"Test label shape = \", TestLabel1.shape)\n",
    "        print(\"Num classes = \", Classes)\n",
    "        \n",
    "        \n",
    "        KAPPA = []\n",
    "        OA = []\n",
    "        AA = []\n",
    "        ELEMENT_ACC = np.zeros((3, Classes))\n",
    "        tf.compat.v1.keras.backend.clear_session()\n",
    "        config = tf.compat.v1.ConfigProto( device_count = {'GPU': 0} ) \n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.compat.v1.Session(config=config) \n",
    "        tf.compat.v1.keras.backend.set_session(sess)\n",
    "        g = tf.Graph()\n",
    "        with g.as_default():\n",
    "            for iter in range(3):\n",
    "        #             tf.compat.v1.set_random_seed(43)\n",
    "        #             np.random.seed(43)\n",
    "\n",
    "                clf = get_model_compiled(NC,Classes)\n",
    "                valdata = (TestPatch1.unsqueeze(-1).cpu().detach().numpy(), keras_to_categorical(TestLabel1.reshape(-1).cpu().detach().numpy(), Classes))\n",
    "                clf.fit(TrainPatch1.unsqueeze(-1).cpu().detach().numpy(), keras_to_categorical(TrainLabel1.reshape(-1).cpu().detach().numpy(), Classes),\n",
    "                                    batch_size=batchsize,\n",
    "                                    epochs=EPOCH,\n",
    "                                    verbose=True,\n",
    "                                    validation_data=valdata,\n",
    "                                    callbacks = [ModelCheckpoint(datasetName+\"/best_model_HSIOnly.h5\", monitor='val_accuracy', verbose=0, save_best_only=True)])\n",
    "\n",
    "\n",
    "\n",
    "                clf = load_model(datasetName+\"/best_model_HSIOnly.h5\")\n",
    "                pred_y = np.argmax(clf.predict(TestPatch1.unsqueeze(-1).cpu().detach().numpy()), axis=1)\n",
    "\n",
    "                y_test = TestLabel1.reshape(-1).cpu().detach().numpy()\n",
    "                oa = accuracy_score(y_test, pred_y)*100\n",
    "                confusion = confusion_matrix(y_test, pred_y)\n",
    "                each_acc, aa = AA_andEachClassAccuracy(confusion)\n",
    "                kappa = cohen_kappa_score(y_test, pred_y)*100\n",
    "                KAPPA.append(kappa)\n",
    "                OA.append(oa)\n",
    "                AA.append(aa)\n",
    "                ELEMENT_ACC[iter, :] = each_acc\n",
    "\n",
    "        print(\"--------\" + datasetName + \" Training Finished-----------\")\n",
    "        record.record_output(OA, AA, KAPPA, ELEMENT_ACC,'./' + datasetName +'/HybridSN_Report_' + datasetName +'.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
